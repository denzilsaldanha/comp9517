{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from  matplotlib import pyplot as plt\n",
    "from collections import defaultdict \n",
    "\n",
    "filenames = glob.glob(\"sequence/*.jpg\")\n",
    "frames = [cv2.imread(img) for img in filenames]\n",
    "\n",
    "\n",
    "frames= np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# import numpy as np\n",
    "\n",
    "# import cv2\n",
    "# import time\n",
    "\n",
    "\n",
    "# class DetectorAPI:\n",
    "#     def __init__(self, path_to_ckpt):\n",
    "#         self.path_to_ckpt = path_to_ckpt\n",
    "\n",
    "#         self.detection_graph = tf.Graph()\n",
    "#         with self.detection_graph.as_default():\n",
    "#             od_graph_def = tf.GraphDef()\n",
    "#             with tf.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n",
    "#                 serialized_graph = fid.read()\n",
    "#                 od_graph_def.ParseFromString(serialized_graph)\n",
    "#                 tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "#         self.default_graph = self.detection_graph.as_default()\n",
    "#         self.sess = tf.Session(graph=self.detection_graph)\n",
    "\n",
    "#         # Definite input and output Tensors for detection_graph\n",
    "#         self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "#         # Each box represents a part of the image where a particular object was detected.\n",
    "#         self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "#         # Each score represent how level of confidence for each of the objects.\n",
    "#         # Score is shown on the result image, together with the class label.\n",
    "#         self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "#         self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "#         self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "#     def processFrame(self, image):\n",
    "#         # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n",
    "#         image_np_expanded = np.expand_dims(image, axis=0)\n",
    "#         # Actual detection.\n",
    "#         start_time = time.time()\n",
    "#         (boxes, scores, classes, num) = self.sess.run(\n",
    "#             [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
    "#             feed_dict={self.image_tensor: image_np_expanded})\n",
    "#         end_time = time.time()\n",
    "\n",
    "#         print(\"Elapsed Time:\", end_time-start_time)\n",
    "\n",
    "#         im_height, im_width,_ = image.shape\n",
    "#         boxes_list = [None for i in range(boxes.shape[1])]\n",
    "#         for i in range(boxes.shape[1]):\n",
    "#             boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
    "#                         int(boxes[0,i,1]*im_width),\n",
    "#                         int(boxes[0,i,2] * im_height),\n",
    "#                         int(boxes[0,i,3]*im_width))\n",
    "\n",
    "#         return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
    "\n",
    "#     def close(self):\n",
    "#         self.sess.close()\n",
    "#         self.default_graph.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_path = 'G:\\\\UNSW\\\\2020 T1\\\\Project\\\\Group_Component\\\\Group_Component\\\\faster_rcnn_inception_v2_coco_2018_01_28\\\\frozen_inference_graph.pb'\n",
    "#     odapi = DetectorAPI(path_to_ckpt=model_path)\n",
    "#     threshold = 0.7\n",
    "    \n",
    "\n",
    "# #     while True:\n",
    "# #         r, img = cap.read()\n",
    "# #         img = cv2.resize(img, (1280, 720))\n",
    "#     frame_array = []\n",
    "#     for i in frames:\n",
    "#         img =i\n",
    "\n",
    "\n",
    "#         # Visualization of the results of a detection.\n",
    "\n",
    "     \n",
    "#         boxes, scores, classes, num = odapi.processFrame(img)\n",
    "\n",
    "#         # Visualization of the results of a detection.\n",
    "\n",
    "#         for i in range(len(boxes)):\n",
    "#             # Class 1 represents human\n",
    "#             if classes[i] == 1 and scores[i] > threshold:\n",
    "#                 box = boxes[i]\n",
    "#                 cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)\n",
    "#         frame_array.append(img)\n",
    "\n",
    "#         cv2.imshow(\"preview\", img)\n",
    "#         if cv2.waitKey(33) == 27:\n",
    "#             break\n",
    "# cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "class CentroidTracker():\n",
    "    def __init__(self, maxDisappeared=50):\n",
    "        # initialize the next unique object ID along with two ordered\n",
    "        # dictionaries used to keep track of mapping a given object\n",
    "        # ID to its centroid and number of consecutive frames it has\n",
    "        # been marked as \"disappeared\", respectively\n",
    "        self.nextObjectID = 0\n",
    "        self.objects = OrderedDict()\n",
    "        self.disappeared = OrderedDict()\n",
    "        # store the number of maximum consecutive frames a given\n",
    "        # object is allowed to be marked as \"disappeared\" until we\n",
    "        # need to deregister the object from tracking\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "    def register(self, centroid):\n",
    "        # when registering an object we use the next available object\n",
    "        # ID to store the centroid\n",
    "        self.objects[self.nextObjectID] = centroid\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.nextObjectID += 1\n",
    "    def deregister(self, objectID):\n",
    "        # to deregister an object ID we delete the object ID from\n",
    "        # both of our respective dictionaries\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "    def update(self, rects):\n",
    "        # check to see if the list of input bounding box rectangles\n",
    "        # is empty\n",
    "        if len(rects) == 0:\n",
    "            # loop over any existing tracked objects and mark them\n",
    "            # as disappeared\n",
    "            for objectID in list(self.disappeared.keys()):\n",
    "                self.disappeared[objectID] += 1\n",
    "                # if we have reached a maximum number of consecutive\n",
    "                # frames where a given object has been marked as\n",
    "                # missing, deregister it\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    "            # return early as there are no centroids or tracking info\n",
    "            # to update\n",
    "            return self.objects\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
    "        # loop over the bounding box rectangles\n",
    "#         print(enumerate(rects))\n",
    "        \n",
    "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
    "            # use the bounding box coordinates to derive the centroid\n",
    "            cX = int((startX + endX) / 2.0)\n",
    "            cY = int((startY + endY) / 2.0)\n",
    "            inputCentroids[i] = (cX, cY)\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i])\n",
    "                        # otherwise, are are currently tracking objects so we need to\n",
    "        # try to match the input centroids to existing object\n",
    "        # centroids\n",
    "        else:\n",
    "            # grab the set of object IDs and corresponding centroids\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = list(self.objects.values())\n",
    "            # compute the distance between each pair of object\n",
    "            # centroids and input centroids, respectively -- our\n",
    "            # goal will be to match an input centroid to an existing\n",
    "            # object centroid\n",
    "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
    "            # in order to perform this matching we must (1) find the\n",
    "            # smallest value in each row and then (2) sort the row\n",
    "            # indexes based on their minimum values so that the row\n",
    "            # with the smallest value is at the *front* of the index\n",
    "            # list\n",
    "            rows = D.min(axis=1).argsort()\n",
    "            # next, we perform a similar process on the columns by\n",
    "            # finding the smallest value in each column and then\n",
    "            # sorting using the previously computed row index list\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "            # in order to determine if we need to update, register,\n",
    "            # or deregister an object we need to keep track of which\n",
    "            # of the rows and column indexes we have already examined\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    "            # loop over the combination of the (row, column) index\n",
    "            # tuples\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                # if we have already examined either the row or\n",
    "                # column value before, ignore it\n",
    "                # val\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    "                # otherwise, grab the object ID for the current row,\n",
    "                # set its new centroid, and reset the disappeared\n",
    "                # counter\n",
    "                objectID = objectIDs[row]\n",
    "                self.objects[objectID] = inputCentroids[col]\n",
    "                self.disappeared[objectID] = 0\n",
    "                # indicate that we have examined each of the row and\n",
    "                # column indexes, respectively\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "                # compute both the row and column index we have NOT yet\n",
    "            # examined\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "                        # in the event that the number of object centroids is\n",
    "            # equal or greater than the number of input centroids\n",
    "            # we need to check and see if some of these objects have\n",
    "            # potentially disappeared\n",
    "            if D.shape[0] >= D.shape[1]:\n",
    "                # loop over the unused row indexes\n",
    "                for row in unusedRows:\n",
    "                    # grab the object ID for the corresponding row\n",
    "                    # index and increment the disappeared counter\n",
    "                    objectID = objectIDs[row]\n",
    "                    self.disappeared[objectID] += 1\n",
    "                    # check to see if the number of consecutive\n",
    "                    # frames the object has been marked \"disappeared\"\n",
    "                    # for warrants deregistering the object\n",
    "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                        self.deregister(objectID)\n",
    "            # otherwise, if the number of input centroids is greater\n",
    "            # than the number of existing object centroids we need to\n",
    "            # register each new input centroid as a trackable object\n",
    "            else:\n",
    "                for col in unusedCols:\n",
    "                    self.register(inputCentroids[col])\n",
    "        # return the set of trackable objects\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class People_In_Box(object):\n",
    "    # Class to detect people in a box\n",
    "    def __init__(self, frame, centers):\n",
    "        self.people_in_box = 0\n",
    "        self.frame = frame\n",
    "        self.centers = centers\n",
    "\n",
    "    # Function to count the people in the box\n",
    "    def count_people(self, x1, y1, x2, y2):\n",
    "        people_in_box = 0\n",
    "\n",
    "        # Extracting the coordinates from the centroids\n",
    "        for x, y in self.centers:\n",
    "            cX = x\n",
    "            cY = y\n",
    "            # Increasing count of people inside the box\n",
    "            if cX > x1 and cX < x2 and cY > y1 and cY < y2:\n",
    "                people_in_box += 1\n",
    "\n",
    "        # Returning the count\n",
    "        return people_in_box\n",
    "    # Function to count the people in the box\n",
    "    def count_people_in_group(self, threshold):\n",
    "        \"\"\"\n",
    "        Function for Task 3 to fix\n",
    "        \"\"\"\n",
    "        # Establish box with threshold to include other nearby people\n",
    "        people_in_group = 0\n",
    "        people_alone = 0\n",
    "\n",
    "        # Extracting the coordinates from the centroids\n",
    "        for x, y in self.centers:\n",
    "            x1 = x - threshold\n",
    "            y1 = y - threshold\n",
    "            x2 = x + threshold\n",
    "            y2 = y + threshold\n",
    "\n",
    "            # Calculating people in group within threshold\n",
    "            people_in_group = self.count_people(x1, y1, x2, y2)\n",
    "            # Differentiating single pedestrian from groups of people\n",
    "            if people_in_group == 1:\n",
    "                people_in_group = 0\n",
    "                people_alone += 1\n",
    "            # Draws the box if a group (more than 1 people) is detected\n",
    "            elif people_in_group > 1:\n",
    "                # Attempt to keep count value consistent throught but unsure if it is working\n",
    "                people_in_group = self.count_people(x1, y1, x2, y2)\n",
    "                # draw a rectangle\n",
    "                cv2.rectangle(\n",
    "                    self.frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "        # Returning the count of people witin a group as well as the frame with the drawn box\n",
    "        return people_in_group, people_alone, self.frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "class DetectorAPI:\n",
    "    def __init__(self, path_to_ckpt):\n",
    "        self.path_to_ckpt = path_to_ckpt\n",
    "\n",
    "        self.detection_graph = tf.Graph()\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        self.default_graph = self.detection_graph.as_default()\n",
    "        self.sess = tf.Session(graph=self.detection_graph)\n",
    "\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "    def processFrame(self, image):\n",
    "        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(image, axis=0)\n",
    "        # Actual detection.\n",
    "        start_time = time.time()\n",
    "        (boxes, scores, classes, num) = self.sess.run(\n",
    "            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
    "            feed_dict={self.image_tensor: image_np_expanded})\n",
    "        end_time = time.time()\n",
    "\n",
    "        #print(\"Elapsed Time:\", end_time-start_time)\n",
    "\n",
    "        im_height, im_width,_ = image.shape\n",
    "        boxes_list = [None for i in range(boxes.shape[1])]\n",
    "        for i in range(boxes.shape[1]):\n",
    "            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
    "                        int(boxes[0,i,1]*im_width),\n",
    "                        int(boxes[0,i,2] * im_height),\n",
    "                        int(boxes[0,i,3]*im_width))\n",
    "\n",
    "        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
    "    \n",
    "    def drawTrail(self, pts_dict, image):\n",
    "        for img_ids in pts_dict.keys():\n",
    "            pts = pts_dict[img_ids]\n",
    "            for i in range(1,len(pts)):\n",
    "                if pts[i - 1] is None or pts[i] is None:\n",
    "                    continue\n",
    "                cv2.line(image, pts[i - 1], pts[i], (0, 0, 255), 2)\n",
    "        return image\n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        self.default_graph.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c4cb8798c4b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mcenters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mpts_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobjectID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0modapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawTrail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpts_dict\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m#         cv2.putText(img,text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-c75e86b7f4ad>\u001b[0m in \u001b[0;36mdrawTrail\u001b[1;34m(self, pts_dict, image)\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'C:\\\\Users\\\\abhij\\\\Desktop\\\\LectureNotes\\\\T1 2020\\\\COMP9517\\\\Group project\\\\comp9517\\\\faster_rcnn_inception_v2_coco_2018_01_28\\\\frozen_inference_graph.pb'\n",
    "#     model_path = 'G:\\\\UNSW\\\\2020 T1\\\\Project\\\\Group_Component\\\\Group_Component\\\\ssd_mobilenet_v1_coco_2018_01_28\\\\frozen_inference_graph.pb'\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    odapi = DetectorAPI(path_to_ckpt=model_path)\n",
    "    threshold = 0.8\n",
    "    \n",
    "    (H, W) = (None, None)\n",
    "    boxd = []\n",
    "    scored = []\n",
    "    classed = []\n",
    "    nums = []\n",
    "#     while True:\n",
    "#         r, img = cap.read()\n",
    "#         img = cv2.resize(img, (1280, 720))\n",
    "    frame_array = []\n",
    "    ct = CentroidTracker()\n",
    "    #Default dictionary of format {id:[[centroid[0],centroid[1]],..], id[...]}\n",
    "    pts_dict = defaultdict(list)\n",
    "    \n",
    "    for i in frames:\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frames.shape[:2]\n",
    "        img =i\n",
    "        # Visualization of the results of a detection.       \n",
    "        boxes, scores, classes, num = odapi.processFrame(img)\n",
    "#         boxd.append(boxes)\n",
    "#         scored.append(scores)\n",
    "#         classed.append(classes)\n",
    "#         nums.append(num)\n",
    "        # Visualization of the results of a detection.\n",
    "#         boxd = []\n",
    "        rects = []\n",
    "        for i in range(len(boxes)):\n",
    "            # Class 1 represents human\n",
    "            if classes[i] == 1  and scores[i] > threshold:\n",
    "                box = boxes[i]\n",
    "                cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,255),2)\n",
    "                rects.append(box)\n",
    "        objects = ct.update(rects)    \n",
    "        centers=[]\n",
    "    # loop over the tracked objects\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            # draw both the ID of the object and the centroid of the\n",
    "            # object on the output frame\n",
    "            text = \"ID {}\".format(objectID)\n",
    "            cv2.putText(img, text, (centroid[1] - 10, centroid[0] - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(img, (centroid[1], centroid[0]), 4, (0, 255, 0), 4)\n",
    "            centers.append([centroid[1],centroid[0]])            \n",
    "            pts_dict[objectID].append((centroid[1], centroid[0]))\n",
    "            img = odapi.drawTrail(pts_dict , img)  \n",
    "            \n",
    "#         cv2.putText(img,text)\n",
    "        people_inside = People_In_Box(img, centers)\n",
    "        peopleInGroup, peopleAlone, img = people_inside.count_people_in_group(35)\n",
    "\n",
    "                # Return the count of the people inside the drawn box\n",
    "#         peopleInBox = people_inside.count_people(\n",
    "#                     x_coord, y_coord, x_coord + width, y_coord + height)\n",
    "                \n",
    "        frame_array.append(img)\n",
    "\n",
    "        cv2.imshow(\"preview\", img)\n",
    "        if cv2.waitKey(33) == 27:\n",
    "            break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class groups(object):\n",
    "    def __init__(self,coords,centers):\n",
    "        self.coords = coords\n",
    "        self.centers  = centers\n",
    "    def people_in_groups(self,threshold,frame):\n",
    "        for i in range(len(centers)-1):\n",
    "            for k in range(i,len(centers)):\n",
    "                if abs(centers[i][0]-center[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, layers = frames[0].shape\n",
    "size = (width,height)\n",
    "out = cv2.VideoWriter('tensor_video_tracking_group.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, size)\n",
    "for i in range(len(frame_array)):\n",
    "    # writing to a image array\n",
    "    out.write(frame_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
